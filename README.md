## Machine Learning at Harbour.Space
This repo contains the materials for Masters [Machine Learning course at Harbour.Space University](https://in.harbour.space/data-science/machine-learning-radoslav-neychev-vladislav-goncharenko/) held in February-March 2020.


Great books:
1. Deep Learning book (classics, really): https://www.deeplearningbook.org
2. The Hundred-page Machine Learning book: [link](http://themlbook.com) (available online, e.g. on the [github](https://github.com/ZakiaSalod/The-Hundred-Page-Machine-Learning-Book))


Additional materials for self-study:
1. Naive Bayesian classifier explained: [link](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)
2. Stanford notes on linear models: [link](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
3. Detailed description of bootstrap procedure: [link](http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/notes/notebootstrap.pdf)
4. Bias-variance tradeoff in more general case: A Unified Bias-Variance Decomposition and its Applications [link](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)
5. Notes on matrix derivatives from Stanford: [link]( http://cs231n.stanford.edu/handouts/derivatives.pdf)

_______________ __We've got till here for now__ ____________________

5.  Great interactive blogpost by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
6.  And great gradient boosted trees playground by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html
7. Shap values repo and explanation: https://github.com/slundberg/shap
8. Kaggle tutorial on feature importances: https://www.kaggle.com/learn/machine-learning-explainability