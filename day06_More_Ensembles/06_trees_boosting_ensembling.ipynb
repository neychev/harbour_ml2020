{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-MpKxz14SSU"
   },
   "source": [
    "## 06 Decision trees and ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*special thanks to YSDA team for provided materials*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWa4yz4qbxkF"
   },
   "source": [
    "![img](https://pbs.twimg.com/media/B13n2VVCIAA0hJS.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EBDdjJ-ay0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zguy6o5Pb3-x"
   },
   "source": [
    "Let's generate a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "tdtf3-9WauYy",
    "outputId": "967ef1f5-6525-447e-e0f3-de9304ea4435"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_toy, y_toy = make_blobs(n_samples=400,\n",
    "                          centers=[[0., 1.], [1., 2.]],\n",
    "                          random_state=14)\n",
    "\n",
    "plt.scatter(X_toy[:, 0], X_toy[:, 1], c=y_toy, alpha=0.8, cmap='bwr')\n",
    "plt.xlabel('X1'), plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9V31PoNcFuO"
   },
   "source": [
    "## Decision trees out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBmN8Ildbe0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yr-JTRwCcDR1"
   },
   "source": [
    "DecisionTreeClassifier has a number of parameters:\n",
    "* `max_depth` – a limit on tree depth (default – no limit)\n",
    "* `min_samples_split` – there should be at least this many samples to split further (default – 2)\n",
    "* `min_samples_leaf` – there should be at least this many samples on one side of a split to consider it valid (default – 1).\n",
    "* `criterion` – 'gini' or 'entropy' – split stuff over this parameter (default : gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "KX2Dm7onbq2s",
    "outputId": "dee5f202-782f-405f-d71f-6e566b09d375"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_leaf=15)\n",
    "clf.fit(X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IM02fOjxcMic"
   },
   "source": [
    "### Plot decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBVDpZ6Y-jB5"
   },
   "source": [
    "Here's a function that makes a 2d decision boundary plot for a given classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGp3PLfW35hF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_decision_surface(\n",
    "                  clf, X, y,\n",
    "                  grid_step=0.02,\n",
    "                  cmap='bwr',\n",
    "                  alpha=0.6,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of clf on X and y, visualize training points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the grid\n",
    "    x_top_left = X.min(axis=0) - 1\n",
    "    x_bottom_right = X.max(axis=0) + 1\n",
    "    grid_x0, grid_x1 = np.meshgrid(\n",
    "         np.arange(x_top_left[0], x_bottom_right[0], grid_step),\n",
    "         np.arange(x_top_left[1], x_bottom_right[1], grid_step)\n",
    "      )\n",
    "    \n",
    "    # Calculate predictions on the grid\n",
    "    y_pred_grid = clf.predict(\n",
    "                        np.stack(\n",
    "                              [\n",
    "                                grid_x0.ravel(),\n",
    "                                grid_x1.ravel()\n",
    "                              ],\n",
    "                              axis=1\n",
    "                            )\n",
    "                      ).reshape(grid_x1.shape)\n",
    "    \n",
    "    # Find optimal contour levels and make a filled\n",
    "    # contour plot of predictions\n",
    "    labels = np.sort(np.unique(y))\n",
    "    labels = np.concatenate([[labels[0] - 1],\n",
    "                             labels,\n",
    "                             [labels[-1] + 1]])\n",
    "    medians = (labels[1:] + labels[:-1]) / 2\n",
    "    plt.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
    "                 levels=medians)\n",
    "    \n",
    "    # Scatter data points on top of the plot,\n",
    "    # with different styles for correct and wrong\n",
    "    # predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    plt.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
    "                marker='o', cmap=cmap, s=10, label='correct')\n",
    "    plt.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
    "                marker='x', cmap=cmap, s=50, label='errors')\n",
    "\n",
    "    # Dummy plot call to print the accuracy in the legend.\n",
    "    plt.plot([], [], ' ',\n",
    "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
    "    \n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9ODNVnC-jB9"
   },
   "source": [
    "Let's apply it to the tree we've fitted above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "-_2BZxCrb0tn",
    "outputId": "cc247feb-3128-4dcf-bc41-3e04e385ab55"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plot_decision_surface(clf, X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RWucAGNcY-t"
   },
   "source": [
    "### Tree depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImiOildL-jCE"
   },
   "source": [
    "First we are going to split our data to train and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6clG_qq-jCE"
   },
   "outputs": [],
   "source": [
    "X_toy_train, X_toy_test, y_toy_train, y_toy_test = \\\n",
    "    train_test_split(X_toy, y_toy, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QeL0PME0-jCI"
   },
   "source": [
    "Now it's your turn to investigate how the decision boundary depends on the tree depth. Maximum tree depth is defined by the `max_depth` parameter. Try out the following values: `[1, 2, 3, 5, 10]`. Make decision boundary plots for both train and test datasets (separately).\n",
    "\n",
    "  > *Hint: you can make a nice plot with multiple columns and rows (see example below).*\n",
    "  \n",
    "```python\n",
    "plt.figure(figsize=(width, height))\n",
    "for i in range(num_rows * num_columns):\n",
    "  plt.subplot(num_rows, num_columns, i + 1)\n",
    "  # subplot numbering starts from 1   ^^^\n",
    "  \n",
    "  # ...\n",
    "  # (do the plotting)\n",
    "plt.show();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ppdUZd6U-jCI",
    "outputId": "728cab8a-77e8-4cd6-d2f6-5e57f5224581"
   },
   "outputs": [],
   "source": [
    "depth_values = [1, 2, 3, 5, 10]\n",
    "\n",
    "num_rows, num_columns = # <YOUR CODE HERE>\n",
    "\n",
    "plt.figure(figsize=(15,25))\n",
    "for i in range(num_rows):\n",
    "    # <YOUR CODE HERE>\n",
    "    plt.subplot(num_rows, num_columns, 2 * i + 1)\n",
    "    # <YOUR CODE HERE>\n",
    "    # subplot numbering starts from 1   ^^^\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lgCWhbEc77C"
   },
   "source": [
    "### Toy multiclass data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48yemIKk-jCQ"
   },
   "source": [
    "Now let's try out a multiclass classification case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "wahVk0jegnb_",
    "outputId": "557098a1-9997-4438-a631-de07e7330812"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/yandexdataschool/MLatImperial2020/raw/master/04_lab/data.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7iUjzyo-jCU"
   },
   "source": [
    "Firstly, we'll load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w2qKojahYhm1",
    "outputId": "7e0092b1-b889-4ddf-b0a3-29f6300ada0b"
   },
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cuuN5L6-jCb"
   },
   "source": [
    "And then split it to train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybPi8TNtYlgV"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.5, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYgX24sd-jCh"
   },
   "source": [
    "Now it's your turn to have a look at the data. Make a 2d scatter plot of the data points.\n",
    "\n",
    " > *Hint: instead of calling `scatter` separately for each class, you can give it a vector of color index values through the `c` parameter (`scatter(x0, x1, c=y, cmap='rainbow'`). The 'rainbow' colormap gives good enough color diversity for the multiclass case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "V5l3oJDs-jCi",
    "outputId": "c670c823-cb05-4f0c-a97e-e3afd519c08b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "#<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6NA1BGH-jCo"
   },
   "source": [
    "Now that we've had a look at the data, let's fit a decision tree on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "-dvkVwtLZjDA",
    "outputId": "c8a323bb-1d8e-4bc1-a226-4d3c05f9c458"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3s03yi0-jCs"
   },
   "source": [
    "and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "XEHFL51JY02v",
    "outputId": "722e05e7-ede5-4372-a559-dfbc1beb2f5f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_decision_surface(clf, X_train, y_train, cmap='rainbow', grid_step=0.2)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_decision_surface(clf, X_test, y_test, cmap='rainbow', grid_step=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtKtjwBDdKZA"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmEoJSDxdHHs"
   },
   "source": [
    "#### We need a better tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwZ4BVxobkiu"
   },
   "source": [
    "Try adjusting the parameters of DecisionTreeClassifier to improve the test accuracy.\n",
    " * Accuracy >= 0.72 - not bad for a start\n",
    " * Accuracy >= 0.75 - better, but not enough\n",
    " * Accuracy >= 0.77 - pretty good\n",
    " * Accuracy >= 0.78 - great! (probably the best result for a single tree)\n",
    " \n",
    "Feel free to modify the DecisionTreeClassifier above instead of re-writing everything.\n",
    "\n",
    "**Note:** some of the parameters you can tune are under the \"Decision trees out of the box\" header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvm8lSZx-K5C"
   },
   "source": [
    "#### Bonus quest\n",
    "Try adding feature transformations using a pipeline and a function transformer, e.g.:\n",
    "```python\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(\n",
    "    FunctionTransformer(lambda X: np.concatenate([X, X**2], axis=1)),\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "```\n",
    "\n",
    "Which transformations should improve the score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0sTlV_msz3H"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvifS1edLmnd"
   },
   "source": [
    "We've talked a lot about the importance of feature scaling. Why aren't we doing it here?\n",
    "\n",
    "Try adding a standard scaler to the pipeline of your best model and check how it affects the result. Can you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "gX25hYFvL8DP",
    "outputId": "43bdcdd7-2146-45e3-909c-2533e0dbd0bb"
   },
   "outputs": [],
   "source": [
    "# <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHT8J3bfLlWM"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmU84QKFs1qx"
   },
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional quest: AdaBoost from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `Adaboost` class** (for classification taks)\n",
    "\n",
    "**Details:**\n",
    "- inherit class from `sklearn.BaseEstimator`;\n",
    "- constructor must support the following parameters: \n",
    "    - `n_estimators` - the number of base models (decision stomps) for the ensemble;\n",
    "    - `max_depth` - max depth for each decision tree (default - `None`);\n",
    "- methods `fit` и `predict` must be implemented;\n",
    "- method `fit` takes design matrix `X` and target vector `y` as inputs (`numpy.ndarray`) and returns `Adaboost` class, which is an ensemble of base estimators, trained using a dataset `(X, y)` given the parameters, passed to the constructor; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Adaboost(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Boosting method that uses a number of weak classifiers in \n",
    "    ensemble to make a strong classifier. This implementation uses DecisionTreeClassifier. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clf: int\n",
    "        The number of weak classifiers that will be used. \n",
    "    max_depth: int\n",
    "        Maximal depth of the decision tree inside the ensemble\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=5, max_depth=1, random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.alphas = []\n",
    "        self.clfs = []\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize all weights as 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "        \n",
    "        self.clfs = []\n",
    "        self.alphas = []\n",
    "        \n",
    "        _eps = 1e-10\n",
    "        # Iterative fitting of the \"weak\" base estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            \n",
    "            clf = <YOUR CODE HERE>\n",
    "            \n",
    "            min_error = <YOUR CODE HERE>\n",
    "            \n",
    "            # Compute `alpha` parameter, which is used to update the weights of the samples from the training set\n",
    "            # `alpha` may be considered as the  algorithm \"performance\" level approximation\n",
    "            alpha = <YOUR CODE HERE>\n",
    "            \n",
    "            # Update the weights of the training set samples\n",
    "            # Misclassified samples get larger weights, than those with correct class predictions\n",
    "            w *= <YOUR CODE HERE>\n",
    "            \n",
    "            # Updated weights normalization\n",
    "            w /= np.sum(w)\n",
    "            \n",
    "            # Save the base estimator and add its `alpha` parameter to the corresponding list\n",
    "            self.clfs.append(clf)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "        \n",
    "        # Get the prediction from the every model, taking into account the corresponding `alpha` value\n",
    "        for alpha, clf in zip(self.alphas, self.clfs):\n",
    "            # Sum up the predictions weighted with the corresponding `alpha` values\n",
    "            y_pred += <YOUR CODE HERE>\n",
    "\n",
    "        # Class prediction is defined as the sign of weighted sum of the base estimator predictions\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the toy dataset to evaluate the ensemble performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# select two different digits to construct the binary classification problem\n",
    "digit1 = 1\n",
    "digit2 = 8\n",
    "idx = np.append(np.where(y == digit1)[0], np.where(y == digit2)[0])\n",
    "y = data.target[idx]\n",
    "\n",
    "# Change class labels to -1 and 1\n",
    "y[y == digit1] = -1\n",
    "y[y == digit2] = 1\n",
    "X = data.data[idx]\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up AdaBoost hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base estimator number\n",
    "n_estimators = 12\n",
    "max_depth = 1\n",
    "\n",
    "# set up base estimator\n",
    "base_clf = DecisionTreeClassifier(random_state=42, max_depth=max_depth)\n",
    "base_clf.fit(train_X, train_Y)\n",
    "\n",
    "# save base estimator performance for future comparison\n",
    "y_pred_base = base_clf.predict(test_X)\n",
    "print (\"Decision Stump accuracy: {:.5f}\".format(accuracy_score(test_Y, y_pred_base)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a reference model using scikit-learn version of AdaBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# use scikit-learn version as a reference\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=base_clf,\n",
    "    n_estimators=n_estimators,\n",
    "    random_state=42,\n",
    "    algorithm='SAMME')\n",
    "\n",
    "ada.fit(train_X, train_Y)\n",
    "y_pred_ada = ada.predict(test_X)\n",
    "\n",
    "sklearn_accuracy = accuracy_score(test_Y, y_pred_ada)\n",
    "print (\"Scikit-learn' AdaBoost with {} estimators accuracy: {:.5f}\".format(n_estimators, sklearn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we are finally using our own class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Adaboost(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "y_pred = clf.predict(test_X)\n",
    "my_accuracy = accuracy_score(test_Y, y_pred)\n",
    "\n",
    "assert np.allclose(my_accuracy, sklearn_accuracy), 'Your implementation accuracy does not match the sklearn version'\n",
    "print (\"My AdaBoost implementation with {} estimators accuracy: {:.5f}\".format(n_estimators, my_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfDOLLV22djx"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtM1TZql2fCP"
   },
   "source": [
    "### Pre-implemented ensembles: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXbycsrJYQTW"
   },
   "source": [
    "One of the most commonly used libraries for gradient boosing is the [XGBoost library](https://xgboost.ai/). Consider reading [this document](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) for an introduction to the algorithm.\n",
    "\n",
    "Here's the [help page](https://xgboost.readthedocs.io/en/latest/parameter.html) listing available parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zamTOiyMZNHD"
   },
   "source": [
    "Let's start by importing the classifier class and the function that plots individual trees as graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocW-MLi90LxD"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eK3C_XPjZtfU"
   },
   "source": [
    "We can now investigate how decision surface depends on the number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ny94kTDqPzPO"
   },
   "outputs": [],
   "source": [
    "for n_estimators in range(1,10):\n",
    "    # create an XGBClassifier with trees of depth 1\n",
    "    # learning rate 0.5 and n_estimators estimators\n",
    "    model = <YOUR CODE HERE> \n",
    "    \n",
    "    # fit this model to the train data\n",
    "    <YOUR CODE HERE> \n",
    "    \n",
    "    print(\"n_estimators = \", n_estimators)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paFmsQONaQfc"
   },
   "source": [
    "And here's how one may use the `plot_tree` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPp5qkVnXJbC"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "plot_tree(model, num_trees=44, ax=ax, dpi='400');\n",
    "#                   ^^^ This parameter selects the\n",
    "#                       tree that you want to plot.\n",
    "#                       Since there's 9 estimators\n",
    "#                       in the last model and 5\n",
    "#                       classes in our data, the total\n",
    "#                       amount of individual trees\n",
    "#                       is 45 (from 0 to 44)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSu9dBVnvawb"
   },
   "source": [
    "<font color='red'>**Warning:**</font> current xgboost implementation is not very safe to typos, i.e. it can silently swallow whatever argument you provide, even if it has no effect, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoh-aaQlv5-R"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(abrakadabra=\"I won't change anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rq_GvLOgwMHW"
   },
   "source": [
    "so be sure to check your spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErviY92bwdBE"
   },
   "source": [
    "Now let's try to improve the score by adjusting the parameters. Here are some of the parameters you may want to try:\n",
    "  - `max_depth` – maximum tree depth,\n",
    "  - `n_estimators` – number of trees (per class),\n",
    "  - `learning_rate` – shrinkage,\n",
    "  - `reg_lambda` – L2 regularization term on weights,\n",
    "  - `subsample` – row random subsampling rate (per tree),\n",
    "  - `colsample_bynode` – column subsampling rate (per node)\n",
    "  - `gamma` – minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "\n",
    "See [this page](https://xgboost.readthedocs.io/en/latest/parameter.html) for more information.\n",
    "\n",
    "  > *Hint: since XGBClassifier has the same interface as sklearn models, you can use GridSearchCV on it if you want.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTt9TgEIwjrZ"
   },
   "outputs": [],
   "source": [
    "model = <YOUR CODE HERE> \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBpPvhuS3Xa0"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mzvrpP_0UnQ"
   },
   "source": [
    "### Pre-implemented ensembles: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECeEe3MH05BB"
   },
   "source": [
    "RandomForest combines bagging and random subspaces: each tree uses a fraction of training samples, and the splits are chosen among subsets of features. Typically this leads to a slightly better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGns4GcZx3kM"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Task: create and fit a random forest with\n",
    "# 100 estimators and at least 5 samples per leaf\n",
    "\n",
    "model = <YOUR CODE>\n",
    "\n",
    "<YOUR CODE>\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhUHlwfkqI1i"
   },
   "source": [
    "## Bonus part: visualizing a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tz9q62nkhBnU"
   },
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "import io\n",
    "import os\n",
    "\n",
    "from sklearn.tree import plot_tree, export_graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "def plot_tree_svg(tree, columns, max_depth):\n",
    "    buf = io.StringIO()\n",
    "    export_graphviz(tree, out_file=buf,\n",
    "                  feature_names=columns,\n",
    "                  max_depth=max_depth)\n",
    "    graph = pydotplus.graph_from_dot_data(buf.getvalue())\n",
    "    return SVG(graph.create_svg())\n",
    "\n",
    "# there are complications running this on windows, so there is a workaround\n",
    "\n",
    "if os.name == 'nt':\n",
    "    plot_tree(clf.clfs[0], feature_names=list(range(64)), max_depth=3)\n",
    "else:\n",
    "    plot_tree_svg(clf.clfs[0], list(range(64)), max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "de_TKyP7JfZ2"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "trees_ensambling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
