{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10: PyTorch practice, hints and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits:\n",
    "* First part is based on YSDA [Practical RL course week04 materials](https://github.com/yandexdataschool/Practical_RL/tree/master/week04_%5Brecap%5D_deep_learning).\n",
    "* Second part is based on PyTorch official tutorials and [this kaggle kernel](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader)\n",
    "* Third part is based on PyTorch tutorial by [Stanford CS 231n course](http://cs231n.stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes today:\n",
    "- Introduction to PyTorch\n",
    "- Automatic gradient computation\n",
    "- Logistic regression (it's a neural network, actually ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "\n",
    "__This notebook__ will teach you to use pytorch low-level core. You can install it [here](http://pytorch.org/).\n",
    "\n",
    "__Pytorch feels__ differently than other frameworks (like tensorflow/theano) on almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tensormancy\n",
    "\n",
    "__1.1 The [_disclaimer_](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)__\n",
    "\n",
    "Let's write another function, this time in polar coordinates:\n",
    "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (6 \\cdot \\theta) ) \\cdot (1 + 0.01 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.5 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (10 + sin(10 \\cdot \\theta))$$\n",
    "\n",
    "\n",
    "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
    "\n",
    "Use torch tensors only: no lists, loops, numpy arrays, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.linspace(-np.pi, np.pi, steps=1000)\n",
    "\n",
    "# compute rho(theta) as per formula above\n",
    "rho = \n",
    "\n",
    "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
    "x = #<your_code_here>\n",
    "y = #<your_code_here>\n",
    "\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.fill(x.numpy(), y.numpy(), color='red')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Using the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget hhttps://raw.githubusercontent.com/neychev/harbour_ml2020/master/day10_Optimization_and_regularization_in_DL/notmnist.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Extracting ...\n",
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "found broken img: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png [it's ok if <10 images are broken]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from notmnist import load_notmnist\n",
    "X_train, y_train, X_test, y_test = load_notmnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data, self.labels, _ ,_  = load_notmnist(path=file_path, test_size=0)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        image = self.data[index].transpose(1, 2, 0)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "found broken img: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png [it's ok if <10 images are broken]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "full_dataset = DatasetMNIST('./notMNIST_small', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access and get data with index by __getitem__(index)\n",
    "img, lab = full_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0,1]:\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(full_dataset[i][0].reshape([28,28]))\n",
    "    plt.title(str(full_dataset[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(full_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use dataloader as iterator by using iter() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at images and labels of batch size by extracting data `.next()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape on batch size = torch.Size([8, 28, 28, 1])\n",
      "labels shape on batch size = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "/Users/nexes/miniconda3/envs/py3_research/lib/python3.6/site-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make grid takes tensor as arg\n",
    "# tensor : (batchsize, channels, height, width)\n",
    "grid = torchvision.utils.make_grid(images.permute([0, 3, 1, 2]))\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "found broken img: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png [it's ok if <10 images are broken]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_dataset_with_transform = DatasetMNIST(\n",
    "    './notMNIST_small',\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape at the first row : torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "img, lab = train_dataset_with_transform.__getitem__(0)\n",
    "\n",
    "print('image shape at the first row : {}'.format(img.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n",
      "images shape on batch size = torch.Size([8, 1, 28, 28])\n",
      "labels shape on batch size = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "train_loader2 = DataLoader(train_dataset_with_transform, batch_size=8, shuffle=True)\n",
    "\n",
    "train_iter2 = iter(train_loader2)\n",
    "print(type(train_iter2))\n",
    "\n",
    "images, labels = train_iter2.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "/Users/nexes/miniconda3/envs/py3_research/lib/python3.6/site-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing several transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to take data augmentation, you have to make List using `torchvision.transforms.Compose`\n",
    "\n",
    "```\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "```\n",
    "\n",
    "\n",
    "this function can convert some image by order within `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeToVector():\n",
    "    def __call__(self, pic):\n",
    "        return pic.view(pic.size(0), -1)[0]\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ReshapeToVector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    ReshapeToVector()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "found broken img: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png [it's ok if <10 images are broken]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_dataset_final = DatasetMNIST(\n",
    "    './notMNIST_small',\n",
    "    transform=new_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n",
      "images shape on batch size = torch.Size([8, 784])\n",
      "labels shape on batch size = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset_final, batch_size=8, shuffle=True)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))\n",
    "\n",
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential()\n",
    "model.add_module('first', nn.Linear(784, 10))\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step #0 | mean loss = 2.366\n",
      "step #10 | mean loss = 1.922\n",
      "step #20 | mean loss = 1.412\n",
      "step #30 | mean loss = 1.152\n",
      "step #40 | mean loss = 0.772\n",
      "step #50 | mean loss = 0.834\n",
      "step #60 | mean loss = 0.625\n",
      "step #70 | mean loss = 0.491\n",
      "step #80 | mean loss = 0.629\n",
      "step #90 | mean loss = 0.794\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # sample 256 random images\n",
    "    x_batch, y_batch = train_iter.next()\n",
    "    \n",
    "    # predict probabilities\n",
    "    y_predicted = model(x_batch)\n",
    "    \n",
    "#     assert y_predicted.dim() == 1, \"did you forget to select first column with [:, 0]\"\n",
    "    \n",
    "    # compute loss, just like before\n",
    "    loss = loss_func(y_predicted, y_batch)### YOUR CODE\n",
    "    \n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Adam step\n",
    "    opt.step()\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # clear gradients\n",
    "    opt.zero_grad()\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "Try to add some additional transformations (e.g. random crop, rotation etc.) and train your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the model (model checkpointing)\n",
    "\n",
    "Now we have trained a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
    "\n",
    "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizerâ€™s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Testing -- you should get a pretty poor performance since the model hasn't learned anything yet.\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a training loop with model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(epoch, save_interval, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            # different from before: saving model checkpoints\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        test()\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save(5, save_interval=500, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
    "# should give you the final model accuracy\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "### More about pytorch:\n",
    "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 torch",
   "language": "python",
   "name": "py3_research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
